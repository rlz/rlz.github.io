---
permalink: /it-consulting/
---

<html>

<head>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
    (function (d, w, c) {
        (w[c] = w[c] || []).push(function() {
            try {
                w.yaCounter49356415 = new Ya.Metrika2({
                    id:49356415,
                    clickmap:true,
                    trackLinks:true,
                    accurateTrackBounce:true,
                    webvisor:true
                });
            } catch(e) { }
        });

        var n = d.getElementsByTagName("script")[0],
            s = d.createElement("script"),
            f = function () { n.parentNode.insertBefore(s, n); };
        s.type = "text/javascript";
        s.async = true;
        s.src = "https://mc.yandex.ru/metrika/tag.js";

        if (w.opera == "[object Opera]") {
            d.addEventListener("DOMContentLoaded", f, false);
        } else { f(); }
    })(document, window, "yandex_metrika_callbacks2");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/49356415" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
<!-- /Yandex.Metrika counter -->

    <title>IT консалтинг</title>
    <link rel="stylesheet" href="/assets/css/it-consulting.css" />
</head>

<body>
    <img src="/assets/images/about-header.jpg" />
    <div class="body">
        <h1>IT консалтинг</h1>
        <div class="name">Дмитрий Масленников, Москва</div>
        <div class="contacts">+7 (985) 318 17 92</div>
        <div class="contacts">WhatsApp: +353 87 46 24 441</div>
        <div class="contacts">E-mail:
            <a href="mailto:maslennikovdm@gmail.com">maslennikovdm@gmail.com</a>
        </div>
        <div class="contacts">LinkedIn:
            <a href="https://www.linkedin.com/in/dmitry-maslennikov-ba36a617/">https://www.linkedin.com/in/dmitry-maslennikov-ba36a617/</a>
        </div>
        <p>
            Приду к вам в офис (Москва) рассмотрю вашу ситуацию и дам советы по улучшению архитектуры вашего приложения
            или сервиса. За консультацию возьму столько, сколько посчитаете
            нужным заплатить по результату работы — вы сами оцениваете полезность встречи и назначаете цену.
        </p>
        <p>
             Имею огромный опыт в (подробнее ниже):
        </p>
        <ul>
            <li>высоко-нагруженных сервисах (миллионы запросов в секунду)</li>
            <li>высокой отказоустойчивости (high availability)</li>
            <li>мониторинге (мониторинг десятков и сотен тысяч серверов и сотен миллионов метрик)</li>
            <li>обработке больших данных (сотни терабайт)</li>
            <li>оптимизация времени отклика (latency)</li>
        </ul>
        <p>
            Мои советы, примененные вами, позволят решить ваши проблемы по масштабированию ваших сервисов, решить проблемы отказоустойчивости
            или уменьшить расходы на инфраструктуру за счет оптимизации архитектуры.
        </p>
        <h2>Мой опыт за последние 4 года</h2>
        <h3>Google Ireland, Site Reliability Engineer, Search Backend Team</h3>
        <ul>
            <li>
            Работал Site Reliability Engineer (SRE, инженер по надежности) в Google в Ирландии. Над центральном проекте Google — Google
            Search.
            </li>
            <li>
                В команде из 30 инженеров мы отвечали за бесперебойную работу бекенда самого популярного поискового сервиса
            в мире.</li>
            <li>В Google я занимался системами мониторинга (сбор и обработка метрик с сотен тысяч серверов), анализом
            вносимых изменений на предмет возможных отказов, развертыванием новых инсталляций поиска в новых датацентрах
            (~40000 физических серверов), планированием размеров необходимой инфраструктуры.</li>
            <li>Как инженер отвечающий за безотказную работу я должен был знать архитектуру всего
            поиска и сервисы, от которых он зависит, а это сотни различных подсистем. Я изучал их возможное влияние на ошибки,
            влияние на скорость выдачи результатов (медленный поиск считается за отказ).</li>
            <li>Я должен был предвидеть какими проблемами
            может грозить то или иное нововведение (Google поиск обновляется еженедельно, то же самое делают многие сервисы
            от которых он зависит). Команда SRE имеет право вето на деплой любого изменения, каждый член команды обладает
            полным доступом к продакшену поиска. Каждый член команды Search SRE может принудительно экстренно остановить
            абсолютно любой сервис внутри Google по собственному решению, если считает, что он в данный момент мешает
            работе поиска.</li>
            <li>Я знаю, как прогнозировать нагрузку, и как подготовиться к перегрузке системы если к
            вам вдруг нахлынули толпы нежданных пользователей.</li>
            <li>Я знаю, что делать и как подготовиться к полному падению датацентра
            или двух. Как сделать архитектуру которая масштабируется горизонтально и на весь мир не на словах, а на практике.</li>
        </ul>
        <h3>
            <a href="https://www.iponweb.com/">IponWeb</a>, Senior System Architect,
            <a href="http://www.bidswitch.com/">Bidswitch</a> Project</h3>
        <ul>
            <li>За год моей работы над Bidswitch он вырос в 8 раз по объему обрабатываемых запросов с 0.5 миллионов запросов в секунду до
            более чем 2-х миллионов запросов в секунду в пике. Для этого пришлось решить ряд проблем с масштабированием балансировщиков
            нагрузки, системы мониторинга, системы обработки и анализа лог файлов (более терабайта сжатых лог-файлов в сутки),
            системы хранения данных и прочего.</li>
            <li>Мной была разработана и внедрена система детального учета расходов на инфраструктуру,
            что позволило оценивать долю конкретного клиента в наших расходах на общую инфраструктуру и выявлять невыгодных
            клиентов, а так же в один момент произвести оптимизацию архитектуры и сэкономить более $100000 в наших расходах
            на инфраструктуру.</li>
            <li>Так же в процессе масштабирования мы столкнулись с проблемой частых отказов нашего хранилища
            данных, что привело к нарушению SLA и грозило потерей репутации и клиентов. Мной было разработан и внедрен собственный
            подход к шардингу данных вместо стандартных компонент доступных на рынке, что позволило полностью решить проблему
            отказов и дополнительно снизить нагрузку по поддержке и обслуживанию системы на DevOps инженеров.</li>
        </ul>
    </div>
    <script type="text/javascript">
        _linkedin_partner_id = "409002";
        window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
        window._linkedin_data_partner_ids.push(_linkedin_partner_id);
        </script><script type="text/javascript">
        (function(){var s = document.getElementsByTagName("script")[0];
        var b = document.createElement("script");
        b.type = "text/javascript";b.async = true;
        b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
        s.parentNode.insertBefore(b, s);})();
        </script>
        <noscript>
        <img height="1" width="1" style="display:none;" alt="" src="https://dc.ads.linkedin.com/collect/?pid=409002&fmt=gif" />
    </noscript>
</body>

</html>